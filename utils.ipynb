{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "utils.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Mount Drive \"\"\"\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEjcoY40txMe",
        "outputId": "847e4098-ce6c-4330-d260-c7191d1b12ce"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from tensorflow import keras\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n"
      ],
      "metadata": {
        "id": "NUtFLmP_Eq64"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gV-2Ik_kfxmV"
      },
      "source": [
        "### Stanford 40"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhJtnW9eCTHd"
      },
      "source": [
        "def onceSF():\n",
        "  #Import dataset\n",
        "  !wget http://vision.stanford.edu/Datasets/Stanford40_JPEGImages.zip\n",
        "  !wget http://vision.stanford.edu/Datasets/Stanford40_ImageSplits.zip\n",
        "\n",
        "  #Unzip it\n",
        "  !unzip Stanford40_JPEGImages.zip -d Stanford40/\n",
        "  !unzip Stanford40_ImageSplits.zip -d Stanford40/\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XECyi77uk9Tf"
      },
      "source": [
        "### TV Human Interaction (TV-HI)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def onceTVHI():\n",
        "\n",
        "  #Download data\n",
        "  !wget http://www.robots.ox.ac.uk/~alonso/data/tv_human_interactions_videos.tar.gz\n",
        "  !wget http://www.robots.ox.ac.uk/~alonso/data/readme.txt\n",
        "\n",
        "  #Untar compressed files and move the readme.txt into TV-HI folder\n",
        "  !mkdir TVHI_data\n",
        "  !tar -xvf  'tv_human_interactions_videos.tar.gz' -C TVHI_data\n",
        "  !mv readme.txt 'TV-HI/readme.txt'"
      ],
      "metadata": {
        "id": "9L_i9Lgq_BRK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main body utils###"
      ],
      "metadata": {
        "id": "tyPoANBbBqNr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Function for sorting the Stanford40 data in a way that can be accessed by Keras data loading function \"\"\"\n",
        "\n",
        "def dataExtractionSF(needDirectories):\n",
        "    onceSF()\n",
        "\n",
        "    with open('Stanford40/ImageSplits/train.txt', 'r') as f:\n",
        "        train_files = list(map(str.strip, f.readlines()))\n",
        "        train_labels = ['_'.join(name.split('_')[:-1]) for name in train_files]\n",
        "    \n",
        "    \n",
        "    with open('Stanford40/ImageSplits/test.txt', 'r') as f:\n",
        "        test_files = list(map(str.strip, f.readlines()))\n",
        "        test_labels = ['_'.join(name.split('_')[:-1]) for name in test_files]\n",
        "\n",
        "        \n",
        "    action_categories = sorted(list(set(['_'.join(name.split('_')[:-1]) for name in train_files])))  \n",
        "\n",
        "    #Split training data here \n",
        "    train_files, validation_files, train_labels, validation_labels = train_test_split(train_files, train_labels, test_size=0.1, random_state=0, stratify=train_labels)\n",
        "\n",
        "    if needDirectories:\n",
        "\n",
        "        print(\"Beginning sorting images...\")\n",
        "            # Specify names of directories for train and test data\n",
        "        dirs_needed = [\"SF_train\", \"SF_test\", \"SF_validation\"]\n",
        "        files_n_labels = [[train_files, train_labels], [test_files, test_labels],[validation_files, validation_labels]]\n",
        "\n",
        "        for s in range(len(dirs_needed)):\n",
        "\n",
        "            os.mkdir(dirs_needed[s]) # make directory each for training and test set\n",
        "\n",
        "            for label in action_categories:\n",
        "                os.mkdir(f\"{dirs_needed[s]}/{label}\") # in each directory make directories for all categories\n",
        "\n",
        "            counter = 0\n",
        "            #Loop through all images and place them in the correct folder\n",
        "            for file in range(len(files_n_labels[s][0])):\n",
        "                label = files_n_labels[s][1][file]\n",
        "                image = cv2.imread(f\"Stanford40/JPEGImages/{files_n_labels[s][0][file]}\")\n",
        "                image_name = f\"{files_n_labels[s][1][file]}_{counter}.jpg\"\n",
        "                print(image_name, label)\n",
        "                path = f'./{dirs_needed[s]}/{label}'\n",
        "                counter += 1\n",
        "                cv2.imwrite(os.path.join(path,image_name), image) #Write image to directory \n",
        "\n",
        "        print(\"Done sorting images!\")\n",
        "\n",
        "    return train_labels, test_labels, validation_labels, action_categories\n",
        "\n",
        "\n",
        "\"\"\" Load the Standford40 dataset \"\"\"\n",
        "\n",
        "def loadSF40(img_size=(224,224), needDirectories=False):\n",
        "\n",
        "    train_labels, test_labels, validation_labels, class_names = dataExtractionSF(needDirectories)\n",
        " \n",
        "    train_ds = keras.utils.image_dataset_from_directory(\n",
        "    directory='SF_train/',\n",
        "    labels='inferred',\n",
        "    label_mode='int',\n",
        "    batch_size=32,\n",
        "    image_size=img_size,\n",
        "    shuffle=True\n",
        "    )\n",
        "\n",
        "    val_ds = keras.utils.image_dataset_from_directory(\n",
        "    directory='SF_validation/',\n",
        "    labels='inferred',\n",
        "    label_mode='int',\n",
        "    batch_size=32,\n",
        "    image_size=img_size,\n",
        "    shuffle=True\n",
        "    )\n",
        "\n",
        "    test_ds = keras.utils.image_dataset_from_directory(\n",
        "    directory='SF_test/',\n",
        "    labels='inferred',\n",
        "    label_mode='int',\n",
        "    batch_size=32,\n",
        "    image_size=img_size\n",
        "    )\n",
        "    \n",
        "    return train_ds, test_ds, val_ds, train_labels, test_labels, validation_labels, class_names\n",
        "\n",
        "\n",
        "\"\"\" Do dataExtraction on TVHI dataset, get the middle frame and sort into directories for easier data loading \"\"\"\n",
        "\n",
        "def dataExtractionTVHI(needDirectories):\n",
        "    onceTVHI()\n",
        "    \n",
        "    set_1_indices = [[2,14,15,16,18,19,20,21,24,25,26,27,28,32,40,41,42,43,44,45,46,47,48,49,50],\n",
        "                    [1,6,7,8,9,10,11,12,13,23,24,25,27,28,29,30,31,32,33,34,35,44,45,47,48],\n",
        "                    [2,3,4,11,12,15,16,17,18,20,21,27,29,30,31,32,33,34,35,36,42,44,46,49,50],\n",
        "                    [1,7,8,9,10,11,12,13,14,16,17,18,22,23,24,26,29,31,35,36,38,39,40,41,42]]\n",
        "    set_2_indices = [[1,3,4,5,6,7,8,9,10,11,12,13,17,22,23,29,30,31,33,34,35,36,37,38,39],\n",
        "                    [2,3,4,5,14,15,16,17,18,19,20,21,22,26,36,37,38,39,40,41,42,43,46,49,50],\n",
        "                    [1,5,6,7,8,9,10,13,14,19,22,23,24,25,26,28,37,38,39,40,41,43,45,47,48],\n",
        "                    [2,3,4,5,6,15,19,20,21,25,27,28,30,32,33,34,37,43,44,45,46,47,48,49,50]]\n",
        "    classes = ['handShake', 'highFive', 'hug', 'kiss']  # we ignore the negative class\n",
        "\n",
        "    # test set\n",
        "    test_files = [f'{classes[c]}_{i:04d}.avi' for c in range(len(classes)) for i in set_1_indices[c]]\n",
        "    test_labels = [f'{classes[c]}' for c in range(len(classes)) for i in set_1_indices[c]]\n",
        "   \n",
        "    # training set\n",
        "    train_files = [f'{classes[c]}_{i:04d}.avi' for c in range(len(classes)) for i in set_2_indices[c]]\n",
        "    train_labels = [f'{classes[c]}' for c in range(len(classes)) for i in set_2_indices[c]]\n",
        "     \n",
        "    #Split training data here \n",
        "    train_files, validation_files, train_labels, validation_labels = train_test_split(train_files, train_labels, test_size=0.15, random_state=0, stratify=train_labels)\n",
        "\n",
        "\n",
        "    if needDirectories:\n",
        "        \n",
        "        print(\"Beginning sorting images...\")\n",
        "            # Specify names of directories for train, validation and test data\n",
        "        dirs_needed = [\"TVHI_train\", \"TVHI_test\", \"TVHI_validation\"]\n",
        "        files_n_labels = [[train_files, train_labels], [test_files, test_labels],[validation_files, validation_labels]]\n",
        "\n",
        "        for s in range(len(dirs_needed)):\n",
        "\n",
        "            os.mkdir(dirs_needed[s]) # make directory each for training, validation and test sets\n",
        "\n",
        "            for label in classes:\n",
        "                os.mkdir(f\"{dirs_needed[s]}/{label}\") # in each directory make directories for all categories\n",
        "\n",
        "            counter = 0\n",
        "            #Loop through all videos, take middle frame and place them in the correct folder\n",
        "            for video in range(len(files_n_labels[s][0])):\n",
        "                label = files_n_labels[s][1][video]\n",
        "                vidcap = cv2.VideoCapture(f'TVHI_data/tv_human_interactions_videos/{files_n_labels[s][0][video]}')\n",
        "                middle_frame = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT)/2)\n",
        "                vidcap.set(cv2.CAP_PROP_POS_FRAMES, middle_frame) #Get the middle frame of the video\n",
        "                success, frame = vidcap.read()\n",
        "                image_name = f\"{files_n_labels[s][1][video]}_{counter}.jpg\"\n",
        "                print(image_name, label)\n",
        "                path = f'./{dirs_needed[s]}/{label}'\n",
        "                counter += 1\n",
        "                cv2.imwrite(os.path.join(path,image_name), frame) #Write image to directory \n",
        "\n",
        "        print(\"Done sorting images!\")\n",
        "    \n",
        "    \n",
        "    return train_labels, test_labels, validation_labels, classes\n",
        "\n",
        "def loadTVHI(img_size=(224,224), needDirectories=False):\n",
        "\n",
        "    train_labels, test_labels, validation_labels, class_names = dataExtractionTVHI(needDirectories)\n",
        " \n",
        "    train_ds = keras.utils.image_dataset_from_directory(\n",
        "    directory='TVHI_train/',\n",
        "    labels='inferred',\n",
        "    label_mode='int',\n",
        "    batch_size=32,\n",
        "    image_size=img_size,\n",
        "    shuffle=True\n",
        "    )\n",
        "\n",
        "    val_ds = keras.utils.image_dataset_from_directory(\n",
        "    directory='TVHI_validation/',\n",
        "    labels='inferred',\n",
        "    label_mode='int',\n",
        "    batch_size=32,\n",
        "    image_size=img_size,\n",
        "    shuffle=True\n",
        "    )\n",
        "\n",
        "    test_ds = keras.utils.image_dataset_from_directory(\n",
        "    directory='TVHI_test/',\n",
        "    labels='inferred',\n",
        "    label_mode='int',\n",
        "    batch_size=32,\n",
        "    image_size=img_size\n",
        "    )\n",
        "    \n",
        "    return train_ds, test_ds, val_ds, train_labels, test_labels, validation_labels, class_names\n",
        "\n",
        "\"\"\" Function for calculating the optical flow with Farnebäck algorithm \"\"\"\n",
        "\n",
        "def opticalFlowCalculator(video_path, img_size=(224,224)):\n",
        "    optical_flow_data = []\n",
        "    \n",
        "    for video in video_path:\n",
        "\n",
        "        vidcap = cv2.VideoCapture(f'../TVHI_data/tV_human_interactions_videos/{video}') # get video\n",
        "        middle_frame = int((vidcap.get(cv2.CAP_PROP_FRAME_COUNT)/2)-8)      # get index of middle frame, set to -8 frames back so that when we take stack of frames, the middle one will be in the middle of the stack\n",
        "        vidcap.set(cv2.CAP_PROP_POS_FRAMES, middle_frame)               # set the video to the middle frame    \n",
        "        success, old_frame = vidcap.read()                              # read image\n",
        "\n",
        "        hsv = np.zeros_like(old_frame) \n",
        "        hsv[...,1] = 255                                                # Set HSV's Value-channel to constant\n",
        "\n",
        "        old_frame = cv2.cvtColor(old_frame, cv2.COLOR_BGR2GRAY)         # Convert to grayscale to fit algorithm (Farneback)\n",
        "        old_frame  = cv2.resize(old_frame, img_size)                   # Resize image to fit the other data\n",
        "\n",
        "        stackOFframes = []\n",
        "        \n",
        "        OF_params = [0.5, 3, 15, 3, 5, 1.2, 0] #default Farnebacks parameters\n",
        "        \n",
        "        for i in range(16): #Loop over 16 frames, middle frame will be middle of stack\n",
        "            success, new_frame = vidcap.read()\n",
        "            if not success:\n",
        "                break\n",
        "            \n",
        "            #Do preprocessing of new frame \n",
        "            new_frame  = cv2.cvtColor(new_frame,cv2.COLOR_BGR2GRAY)\n",
        "            new_frame  = cv2.resize(new_frame, img_size)\n",
        "\n",
        "            flow = cv2.calcOpticalFlowFarneback(old_frame,new_frame, None, OF_params)   # calculate the optical flow for each pixel in the frame with Farneback\n",
        "\n",
        "            stackOFframes.append(flow)           # add the stack of 16 frames to list\n",
        "\n",
        "            old_frame = new_frame               # update the previous frame to current frame\n",
        "                \n",
        "        optical_flow_data.append(np.asarray(stackOFframes))     #make the stack of frames into an np array and store in general optical flow data list\n",
        "    \n",
        "    return optical_flow_data\n",
        "\n",
        "\n",
        "\"\"\" the Optical Flow input to the CNN \"\"\"\n",
        "\n",
        "def opticalFlowInput():\n",
        "    \n",
        "    #Take relevant data and create test and training set (set 1 = test set,set 2 = training set)\n",
        "    set_1_indices = [[2,14,15,16,18,19,20,21,24,25,26,27,28,32,40,41,42,43,44,45,46,47,48,49,50],\n",
        "                 [1,6,7,8,9,10,11,12,13,23,24,25,27,28,29,30,31,32,33,34,35,44,45,47,48],\n",
        "                 [2,3,4,11,12,15,16,17,18,20,21,27,29,30,31,32,33,34,35,36,42,44,46,49,50],\n",
        "                 [1,7,8,9,10,11,12,13,14,16,17,18,22,23,24,26,29,31,35,36,38,39,40,41,42]]\n",
        "    set_2_indices = [[1,3,4,5,6,7,8,9,10,11,12,13,17,22,23,29,30,31,33,34,35,36,37,38,39],\n",
        "                    [2,3,4,5,14,15,16,17,18,19,20,21,22,26,36,37,38,39,40,41,42,43,46,49,50],\n",
        "                    [1,5,6,7,8,9,10,13,14,19,22,23,24,25,26,28,37,38,39,40,41,43,45,47,48],\n",
        "                    [2,3,4,5,6,15,19,20,21,25,27,28,30,32,33,34,37,43,44,45,46,47,48,49,50]]\n",
        "    classes = ['handShake', 'highFive', 'hug', 'kiss']  # we ignore the negative class\n",
        "\n",
        "    # test set\n",
        "    set_1 = [f'{classes[c]}_{i:04d}.avi' for c in range(len(classes)) for i in set_1_indices[c]]\n",
        "    set_1_label = [f'{classes[c]}' for c in range(len(classes)) for i in set_1_indices[c]]\n",
        "    print(f'Set 1 to be used for test ({len(set_1)}):\\n\\t{set_1}')\n",
        "    print(f'Set 1 labels ({len(set_1_label)}):\\n\\t{set_1_label}\\n')\n",
        "\n",
        "    # training set\n",
        "    train_files = [f'{classes[c]}_{i:04d}.avi' for c in range(len(classes)) for i in set_2_indices[c]]\n",
        "    train_labels = [f'{classes[c]}' for c in range(len(classes)) for i in set_2_indices[c]]\n",
        "    print(f'Set 2 to be used for train and validation ({len(train_files)}):\\n\\t{train_files}')\n",
        "    print(f'Set 2 labels ({len(train_labels)}):\\n\\t{train_labels}')\n",
        "    \n",
        "    \n",
        "    training_data = opticalFlowCalculator(train_files)\n",
        "    testing_data = opticalFlowCalculator(set_1)\n",
        "    train_labels = train_labels\n",
        "    test_labels = set_1_label\n",
        "    \n",
        "    return (training_data, train_labels, testing_data, test_labels)\n",
        "\n",
        "\"\"\"   Data augmentation and Normalisation \"\"\"\n",
        "def dataAugmentation():\n",
        "\n",
        "    img_augmentation = Sequential(\n",
        "    [\n",
        "        layers.Rescaling(scale=1./255),\n",
        "        layers.RandomRotation(factor=0.15),\n",
        "        layers.RandomTranslation(height_factor=0.1, width_factor=0.1),\n",
        "        layers.RandomFlip(mode=\"horizontal\"),\n",
        "        layers.RandomContrast(factor=0.1),\n",
        "        layers.RandomZoom(0.1)\n",
        "    ],\n",
        "    name=\"img_augmentation\",\n",
        "    )\n",
        "\n",
        "    return img_augmentation\n",
        "\n",
        "\n",
        "\"\"\" Function for plotting accuracy\"\"\"\n",
        "\n",
        "def plotAccuracy(title, train_acc, val_acc):\n",
        "    plt.title(title)\n",
        "    plt.plot(train_acc)\n",
        "    plt.plot(val_acc)\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.legend(['train', 'val'], loc = 'upper left')\n",
        "    plt.ylim([0, 1])\n",
        "    plt.show()\n",
        "\n",
        "\"\"\" Function for plotting loss\"\"\"\n",
        "\n",
        "def plotLoss(title, train_loss, val_loss):\n",
        "    plt.title(title)\n",
        "    plt.plot(train_loss)\n",
        "    plt.plot(val_loss)\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.legend(['train', 'val'], loc = 'lower left')\n",
        "    plt.ylim([0, 5])\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "bPpK5_ygZ8Wy"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}