{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "utils.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Mount Drive \"\"\"\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEjcoY40txMe",
        "outputId": "27b96fcd-7967-45c7-894a-c63ba8131129"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main body utils###"
      ],
      "metadata": {
        "id": "tyPoANBbBqNr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "\"\"\" Load the Standford40 dataset, do image resizing \"\"\"\n",
        "\n",
        "def loadSF40(img_size=(224,224)):\n",
        " \n",
        "    with open('Stanford40/ImageSplits/train.txt', 'r') as f:\n",
        "        train_files = list(map(str.strip, f.readlines()))\n",
        "        train_labels = ['_'.join(name.split('_')[:-1]) for name in train_files]\n",
        "\n",
        "    with open('Stanford40/ImageSplits/test.txt', 'r') as f:\n",
        "        test_files = list(map(str.strip, f.readlines()))\n",
        "        test_labels = ['_'.join(name.split('_')[:-1]) for name in test_files]\n",
        "        \n",
        "    action_categories = sorted(list(set(['_'.join(name.split('_')[:-1]) for name in train_files])))\n",
        "    print(f'Action categories ({len(action_categories)}):\\n{action_categories}')\n",
        "    \n",
        "    \n",
        "      #Read images, resize to correct size and put into list\n",
        "    SF_training_set = np.empty((len(train_files), img_size[0],img_size[1], 3))\n",
        "    for i in range(len(train_files)):\n",
        "        img = cv2.imread(f'Stanford40/JPEGImages/{train_files[i]}')\n",
        "        img = cv2.resize(img, img_size)\n",
        "        np.asarray(img)\n",
        "        SF_training_set[i] = img\n",
        "      \n",
        "\n",
        "    SF_test_set = np.empty((len(test_files), img_size[0],img_size[1], 3))\n",
        "    for im in range(len(test_files)):\n",
        "        img = cv2.imread(f'Stanford40/JPEGImages/{test_files[i]}')\n",
        "        img = cv2.resize(img, img_size)\n",
        "        np.asarray(img)\n",
        "        SF_test_set[i] = img\n",
        "    \n",
        "    return (SF_training_set, train_labels, SF_test_set, test_labels, action_categories)\n",
        "\n",
        "\n",
        "\"\"\" Load the TVHI dataset, do data preprocessing \"\"\"\n",
        "\n",
        "def loadTVHIData(img_size=(224,224)):\n",
        "    \n",
        "    set_1_indices = [[2,14,15,16,18,19,20,21,24,25,26,27,28,32,40,41,42,43,44,45,46,47,48,49,50],\n",
        "                    [1,6,7,8,9,10,11,12,13,23,24,25,27,28,29,30,31,32,33,34,35,44,45,47,48],\n",
        "                    [2,3,4,11,12,15,16,17,18,20,21,27,29,30,31,32,33,34,35,36,42,44,46,49,50],\n",
        "                    [1,7,8,9,10,11,12,13,14,16,17,18,22,23,24,26,29,31,35,36,38,39,40,41,42]]\n",
        "    set_2_indices = [[1,3,4,5,6,7,8,9,10,11,12,13,17,22,23,29,30,31,33,34,35,36,37,38,39],\n",
        "                    [2,3,4,5,14,15,16,17,18,19,20,21,22,26,36,37,38,39,40,41,42,43,46,49,50],\n",
        "                    [1,5,6,7,8,9,10,13,14,19,22,23,24,25,26,28,37,38,39,40,41,43,45,47,48],\n",
        "                    [2,3,4,5,6,15,19,20,21,25,27,28,30,32,33,34,37,43,44,45,46,47,48,49,50]]\n",
        "    classes = ['handShake', 'highFive', 'hug', 'kiss']  # we ignore the negative class\n",
        "\n",
        "    # test set\n",
        "    set_1 = [f'{classes[c]}_{i:04d}.avi' for c in range(len(classes)) for i in set_1_indices[c]]\n",
        "    set_1_label = [f'{classes[c]}' for c in range(len(classes)) for i in set_1_indices[c]]\n",
        "    print(f'Set 1 to be used for test ({len(set_1)}):\\n\\t{set_1}')\n",
        "    print(f'Set 1 labels ({len(set_1_label)}):\\n\\t{set_1_label}\\n')\n",
        "\n",
        "    # training set\n",
        "    set_2 = [f'{classes[c]}_{i:04d}.avi' for c in range(len(classes)) for i in set_2_indices[c]]\n",
        "    set_2_label = [f'{classes[c]}' for c in range(len(classes)) for i in set_2_indices[c]]\n",
        "    print(f'Set 2 to be used for train and validation ({len(set_2)}):\\n\\t{set_2}')\n",
        "    print(f'Set 2 labels ({len(set_2_label)}):\\n\\t{set_2_label}')\n",
        "    \n",
        "    \n",
        "    # Take middle frame from each video in TVHI dataset\n",
        "    TVHI_training_set = []\n",
        "    for video in set_2:\n",
        "        vidcap = cv2.VideoCapture(f'../TVHI_data/tV_human_interactions_videos/{video}')\n",
        "        middle_frame = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT)/2)\n",
        "        vidcap.set(cv2.CAP_PROP_POS_FRAMES, middle_frame) #Get the middle frame of the video\n",
        "        success, image = vidcap.read()\n",
        "        if success:\n",
        "            frame = cv2.resize(frame, img_size)\n",
        "            TVHI_training_set.append(frame)\n",
        "            \n",
        "    TVHI_test_set = []\n",
        "    for video in set_1:\n",
        "        vidcap = cv2.VideoCapture(f'../TVHI_data/tV_human_interactions_videos/{video}')\n",
        "        middle_frame = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT)/2)\n",
        "        vidcap.set(cv2.CAP_PROP_POS_FRAMES, middle_frame)\n",
        "        success, image = vidcap.read()\n",
        "        if success:\n",
        "            TVHI_test_set.append(image)\n",
        "    \n",
        "    train_labels = set_2_label\n",
        "    test_labels = set_1_label\n",
        "    \n",
        "    return (TVHI_training_set, train_labels, TVHI_test_set, test_labels, classes) \n",
        "\n",
        "\n",
        "\n",
        "\"\"\" Function for calculating the optical flow with Farneb√§ck algorithm \"\"\"\n",
        "\n",
        "def opticalFlowCalculator(video_path, img_size=(224,224)):\n",
        "    optical_flow_data = []\n",
        "    \n",
        "    for video in video_path:\n",
        "\n",
        "        vidcap = cv2.VideoCapture(f'../TVHI_data/tV_human_interactions_videos/{video}') # get video\n",
        "        middle_frame = int((vidcap.get(cv2.CAP_PROP_FRAME_COUNT)/2)-8)      # get index of middle frame, set to -8 frames back so that when we take stack of frames, the middle one will be in the middle of the stack\n",
        "        vidcap.set(cv2.CAP_PROP_POS_FRAMES, middle_frame)               # set the video to the middle frame    \n",
        "        success, old_frame = vidcap.read()                              # read image\n",
        "\n",
        "        hsv = np.zeros_like(old_frame) \n",
        "        hsv[...,1] = 255                                                # Set HSV's Value-channel to constant\n",
        "\n",
        "        old_frame = cv2.cvtColor(old_frame, cv2.COLOR_BGR2GRAY)         # Convert to grayscale to fit algorithm (Farneback)\n",
        "        old_frame  = cv2.resize(old_frame, img_size)                   # Resize image to fit the other data\n",
        "\n",
        "        stackOFframes = []\n",
        "        \n",
        "        OF_params = [0.5, 3, 15, 3, 5, 1.2, 0] #default Farnebacks parameters\n",
        "        \n",
        "        for i in range(16): #Loop over 16 frames, middle frame will be middle of stack\n",
        "            success, new_frame = vidcap.read()\n",
        "            if not success:\n",
        "                break\n",
        "            \n",
        "            #Do preprocessing of new frame \n",
        "            new_frame  = cv2.cvtColor(new_frame,cv2.COLOR_BGR2GRAY)\n",
        "            new_frame  = cv2.resize(new_frame, img_size)\n",
        "\n",
        "            flow = cv2.calcOpticalFlowFarneback(old_frame,new_frame, None, OF_params)   # calculate the optical flow for each pixel in the frame with Farneback\n",
        "\n",
        "            stackOFframes.append(flow)           # add the stack of 16 frames to list\n",
        "\n",
        "            old_frame = new_frame               # update the previous frame to current frame\n",
        "                \n",
        "        optical_flow_data.append(np.asarray(stackOFframes))     #make the stack of frames into an np array and store in general optical flow data list\n",
        "    \n",
        "    return optical_flow_data\n",
        "\n",
        "\n",
        "\"\"\" the Optical Flow input to the CNN \"\"\"\n",
        "\n",
        "def opticalFlowInput():\n",
        "    \n",
        "    #Take relevant data and create test and training set (set 1 = test set,set 2 = training set)\n",
        "    set_1_indices = [[2,14,15,16,18,19,20,21,24,25,26,27,28,32,40,41,42,43,44,45,46,47,48,49,50],\n",
        "                 [1,6,7,8,9,10,11,12,13,23,24,25,27,28,29,30,31,32,33,34,35,44,45,47,48],\n",
        "                 [2,3,4,11,12,15,16,17,18,20,21,27,29,30,31,32,33,34,35,36,42,44,46,49,50],\n",
        "                 [1,7,8,9,10,11,12,13,14,16,17,18,22,23,24,26,29,31,35,36,38,39,40,41,42]]\n",
        "    set_2_indices = [[1,3,4,5,6,7,8,9,10,11,12,13,17,22,23,29,30,31,33,34,35,36,37,38,39],\n",
        "                    [2,3,4,5,14,15,16,17,18,19,20,21,22,26,36,37,38,39,40,41,42,43,46,49,50],\n",
        "                    [1,5,6,7,8,9,10,13,14,19,22,23,24,25,26,28,37,38,39,40,41,43,45,47,48],\n",
        "                    [2,3,4,5,6,15,19,20,21,25,27,28,30,32,33,34,37,43,44,45,46,47,48,49,50]]\n",
        "    classes = ['handShake', 'highFive', 'hug', 'kiss']  # we ignore the negative class\n",
        "\n",
        "    # test set\n",
        "    set_1 = [f'{classes[c]}_{i:04d}.avi' for c in range(len(classes)) for i in set_1_indices[c]]\n",
        "    set_1_label = [f'{classes[c]}' for c in range(len(classes)) for i in set_1_indices[c]]\n",
        "    print(f'Set 1 to be used for test ({len(set_1)}):\\n\\t{set_1}')\n",
        "    print(f'Set 1 labels ({len(set_1_label)}):\\n\\t{set_1_label}\\n')\n",
        "\n",
        "    # training set\n",
        "    set_2 = [f'{classes[c]}_{i:04d}.avi' for c in range(len(classes)) for i in set_2_indices[c]]\n",
        "    set_2_label = [f'{classes[c]}' for c in range(len(classes)) for i in set_2_indices[c]]\n",
        "    print(f'Set 2 to be used for train and validation ({len(set_2)}):\\n\\t{set_2}')\n",
        "    print(f'Set 2 labels ({len(set_2_label)}):\\n\\t{set_2_label}')\n",
        "    \n",
        "    \n",
        "    training_data = opticalFlowCalculator(set_2)\n",
        "    testing_data = opticalFlowCalculator(set_1)\n",
        "    train_labels = set_2_label\n",
        "    test_labels = set_1_label\n",
        "    \n",
        "    return (training_data, train_labels, testing_data, test_labels)"
      ],
      "metadata": {
        "id": "NUtFLmP_Eq64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gV-2Ik_kfxmV"
      },
      "source": [
        "### Stanford 40"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhJtnW9eCTHd",
        "outputId": "ad438adb-09c4-46db-e3a4-357f810741a9"
      },
      "source": [
        "def onceSF():\n",
        "  #Import dataset\n",
        "  !wget http://vision.stanford.edu/Datasets/Stanford40_JPEGImages.zip\n",
        "  !wget http://vision.stanford.edu/Datasets/Stanford40_ImageSplits.zip\n",
        "\n",
        "  #Unzip it\n",
        "  !unzip Stanford40_JPEGImages.zip -d Stanford40/\n",
        "  !unzip Stanford40_ImageSplits.zip -d Stanford40/\n",
        "\n",
        "  #Load dataset\n",
        "  with open('Stanford40/ImageSplits/train.txt', 'r') as f:\n",
        "    train_files = list(map(str.strip, f.readlines()))\n",
        "    train_labels = ['_'.join(name.split('_')[:-1]) for name in train_files]\n",
        "    print(f'Train files ({len(train_files)}):\\n\\t{train_files}')\n",
        "    print(f'Train labels ({len(train_labels)}):\\n\\t{train_labels}\\n')\n",
        "\n",
        "  with open('Stanford40/ImageSplits/test.txt', 'r') as f:\n",
        "      test_files = list(map(str.strip, f.readlines()))\n",
        "      test_labels = ['_'.join(name.split('_')[:-1]) for name in test_files]\n",
        "      print(f'Test files ({len(test_files)}):\\n\\t{test_files}')\n",
        "      print(f'Test labels ({len(test_labels)}):\\n\\t{test_labels}\\n')\n",
        "      \n",
        "  action_categories = sorted(list(set(['_'.join(name.split('_')[:-1]) for name in train_files])))\n",
        "  print(f'Action categories ({len(action_categories)}):\\n{action_categories}')\n",
        "\n",
        "  #Visualise image in dataset\n",
        "  # import cv2\n",
        "  # from google.colab.patches import cv2_imshow\n",
        "\n",
        "  # image_no = 234  # change this to a number between [0, 3999] and you can see a different training image\n",
        "  # img = cv2.imread(f'Stanford40/JPEGImages/{train_files[image_no]}')\n",
        "  # cv2_imshow(img)\n",
        "  # print(f'An image with the label - {train_labels[image_no]}')\n",
        "\n",
        "  return True\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-02 10:47:05--  http://vision.stanford.edu/Datasets/Stanford40_JPEGImages.zip\n",
            "Resolving vision.stanford.edu (vision.stanford.edu)... 171.64.68.10\n",
            "Connecting to vision.stanford.edu (vision.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 304771808 (291M) [application/zip]\n",
            "Saving to: ‚ÄòStanford40_JPEGImages.zip‚Äô\n",
            "\n",
            "Stanford40_JPEGImag 100%[===================>] 290.65M  12.2MB/s    in 28s     \n",
            "\n",
            "2022-04-02 10:47:32 (10.5 MB/s) - ‚ÄòStanford40_JPEGImages.zip‚Äô saved [304771808/304771808]\n",
            "\n",
            "--2022-04-02 10:47:33--  http://vision.stanford.edu/Datasets/Stanford40_ImageSplits.zip\n",
            "Resolving vision.stanford.edu (vision.stanford.edu)... 171.64.68.10\n",
            "Connecting to vision.stanford.edu (vision.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 62010 (61K) [application/zip]\n",
            "Saving to: ‚ÄòStanford40_ImageSplits.zip‚Äô\n",
            "\n",
            "Stanford40_ImageSpl 100%[===================>]  60.56K   285KB/s    in 0.2s    \n",
            "\n",
            "2022-04-02 10:47:33 (285 KB/s) - ‚ÄòStanford40_ImageSplits.zip‚Äô saved [62010/62010]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XECyi77uk9Tf"
      },
      "source": [
        "### TV Human Interaction (TV-HI)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def onceTVHI():\n",
        "\n",
        "  #Download data\n",
        "  !wget http://www.robots.ox.ac.uk/~alonso/data/tv_human_interactions_videos.tar.gz\n",
        "  !wget http://www.robots.ox.ac.uk/~alonso/data/readme.txt\n",
        "\n",
        "  #Untar compressed files and move the readme.txt into TV-HI folder\n",
        "  !mkdir TV-HI\n",
        "  !tar -xvf  'tv_human_interactions_videos.tar.gz' -C TV-HI\n",
        "  !mv readme.txt 'TV-HI/readme.txt'\n",
        "\n",
        "  #Copty set1 and set2 indices to create lists\n",
        "  set_1_indices = [[2,14,15,16,18,19,20,21,24,25,26,27,28,32,40,41,42,43,44,45,46,47,48,49,50],\n",
        "                 [1,6,7,8,9,10,11,12,13,23,24,25,27,28,29,30,31,32,33,34,35,44,45,47,48],\n",
        "                 [2,3,4,11,12,15,16,17,18,20,21,27,29,30,31,32,33,34,35,36,42,44,46,49,50],\n",
        "                 [1,7,8,9,10,11,12,13,14,16,17,18,22,23,24,26,29,31,35,36,38,39,40,41,42]]\n",
        "  set_2_indices = [[1,3,4,5,6,7,8,9,10,11,12,13,17,22,23,29,30,31,33,34,35,36,37,38,39],\n",
        "                  [2,3,4,5,14,15,16,17,18,19,20,21,22,26,36,37,38,39,40,41,42,43,46,49,50],\n",
        "                  [1,5,6,7,8,9,10,13,14,19,22,23,24,25,26,28,37,38,39,40,41,43,45,47,48],\n",
        "                  [2,3,4,5,6,15,19,20,21,25,27,28,30,32,33,34,37,43,44,45,46,47,48,49,50]]\n",
        "  classes = ['handShake', 'highFive', 'hug', 'kiss']  # we ignore the negative class\n",
        "\n",
        "  # test set\n",
        "  set_1 = [f'{classes[c]}_{i:04d}.avi' for c in range(len(classes)) for i in set_1_indices[c]]\n",
        "  set_1_label = [f'{classes[c]}' for c in range(len(classes)) for i in set_1_indices[c]]\n",
        "  #print(f'Set 1 to be used for test ({len(set_1)}):\\n\\t{set_1}')\n",
        "  #print(f'Set 1 labels ({len(set_1_label)}):\\n\\t{set_1_label}\\n')\n",
        "\n",
        "  # training set\n",
        "  set_2 = [f'{classes[c]}_{i:04d}.avi' for c in range(len(classes)) for i in set_2_indices[c]]\n",
        "  set_2_label = [f'{classes[c]}' for c in range(len(classes)) for i in set_2_indices[c]]\n",
        "  #print(f'Set 2 to be used for train and validation ({len(set_2)}):\\n\\t{set_2}')\n",
        "  #print(f'Set 2 labels ({len(set_2_label)}):\\n\\t{set_2_label}')\n",
        "  print(\"Done loading set 1 and 2!\")\n",
        "\n",
        "\n",
        "  \"\"\"Uncomment below code for visualisation of video\"\"\"\n",
        "  # from moviepy.editor import *\n",
        "\n",
        "  # video_no = 98  # change this to a number between [0, 100] and you can see a different training video from Set 2\n",
        "\n",
        "  # clip=VideoFileClip(f'TV-HI/tv_human_interactions_videos/{set_2[video_no]}')\n",
        "  # print(f'\\n\\nA video with the label - {set_2_label[video_no]}\\n')\n",
        "  # clip.ipython_display(width=280)\n",
        "  return True\n"
      ],
      "metadata": {
        "id": "9L_i9Lgq_BRK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}