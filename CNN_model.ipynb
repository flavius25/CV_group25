{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install kora -q\n",
        "from kora import drive\n",
        "drive.link_nbs()"
      ],
      "metadata": {
        "id": "Pcsu2gb3IpJw"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import utils"
      ],
      "metadata": {
        "id": "yPagwXQbJIMw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c75fe1cf-9312-4585-90eb-5161962f33e1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "importing Jupyter notebook from /nbs/utils.ipynb\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SF_notloaded = True   #set to True after first run once you have your files in the Files folder of Colab "
      ],
      "metadata": {
        "id": "HBRlCZcI9eA0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not SF_notloaded:\n",
        "  utils.onceSF()"
      ],
      "metadata": {
        "id": "p-WbTTXj9Y2-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install efficientnet"
      ],
      "metadata": {
        "id": "WNGiwI1zs0tX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "S_f01t8FEah9"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import TensorBoard, LearningRateScheduler\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "import efficientnet.keras as efn\n",
        "from keras import layers\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Global variables\n",
        "IMG_SIZE = (224,224)\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "#Load the dataset which has already been preprocessed\n",
        "SF_training_set, train_labels, SF_test_set, test_labels, class_names = utils.loadSF40(img_size =  IMG_SIZE)\n",
        "\n",
        "#Split the trainingset to obtain 10% stratified validation set\n",
        "train_images, validation_images, train_labels, validation_labels = train_test_split(SF_training_set, train_labels, test_size=0.1, random_state=0, stratify=train_labels)\n"
      ],
      "metadata": {
        "id": "3azVE7J5GtU3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fab62396-c74d-4f34-cd90-8d66459fdd22"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lengt of train_files:  4000\n",
            "Action categories (40):\n",
            "['applauding', 'blowing_bubbles', 'brushing_teeth', 'cleaning_the_floor', 'climbing', 'cooking', 'cutting_trees', 'cutting_vegetables', 'drinking', 'feeding_a_horse', 'fishing', 'fixing_a_bike', 'fixing_a_car', 'gardening', 'holding_an_umbrella', 'jumping', 'looking_through_a_microscope', 'looking_through_a_telescope', 'phoning', 'playing_guitar', 'playing_violin', 'pouring_liquid', 'pushing_a_cart', 'reading', 'riding_a_bike', 'riding_a_horse', 'rowing_a_boat', 'running', 'shooting_an_arrow', 'smoking', 'taking_photos', 'texting_message', 'throwing_frisby', 'using_a_computer', 'walking_the_dog', 'washing_dishes', 'watching_TV', 'waving_hands', 'writing_on_a_board', 'writing_on_a_book']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"   Data augmentation and Normalisation \"\"\"\n",
        "# data augmentation generator defining the augmentations and data-preprocessing to be made\n",
        "data_generator = ImageDataGenerator(\n",
        "        rescale=1.0/255.0, #normalising pixel values to range 0-1\n",
        "        rotation_range=20, # rotation\n",
        "        width_shift_range=0.2, # horizontal shift\n",
        "        height_shift_range=0.2, # vertical shift\n",
        "        zoom_range=0.2, # zoom\n",
        "        horizontal_flip=True, # horizontal flip\n",
        "        brightness_range=[0.5,1.2]  # brightness\n",
        "        )\n",
        "\n",
        "#Create iterators to pass to the model during training\n",
        "train_iterator = data_generator.flow(train_images, train_labels, batch_size=64)\n",
        "validation_iterator =  data_generator.flow(validation_images, validation_labels, batch_size=64)\n",
        "test_iterator = data_generator.flow(SF_test_set, test_labels, batch_size=64)\n",
        "\n",
        "\n",
        "\"\"\" This is how we will fit it with the model \"\"\"\n",
        "# fit model with generator\n",
        "# model.fit_generator(train_iterator, steps_per_epoch=len(train_iterator), epochs=5)\n",
        "# # evaluate model\n",
        "# _, acc = model.evaluate_generator(test_iterator, steps=len(test_iterator), verbose=0)\n",
        "\n",
        "# https://machinelearningmastery.com/how-to-normalize-center-and-standardize-images-with-the-imagedatagenerator-in-keras/ \n",
        "\n",
        "\"\"\" One-hot encoding \"\"\"\n",
        "\n",
        "def onehot_encoding(image, label):\n",
        "    label = tf.one_hot(label, class_names)\n",
        "    return image, label\n",
        "\n",
        "\n",
        "train_images = train_images.map(\n",
        "    onehot_encoding, num_parallel_calls=tf.data.AUTOTUNE\n",
        ")\n",
        "train_images = train_images.batch(batch_size=BATCH_SIZE, drop_remainder=True)\n",
        "train_images = train_images.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "SF_test_set = SF_test_set.map(onehot_encoding)\n",
        "SF_test_set = SF_test_set.batch(batch_size=BATCH_SIZE, drop_remainder=True)\n"
      ],
      "metadata": {
        "id": "G5GaJR8fGwxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = layers.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
        "\n",
        "outputs = efn.EfficientNetB0(include_top=True, weights=None, classes=class_names)\n",
        "\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# hist = model.fit(ds_train, epochs=epochs, validation_data=ds_test, verbose=2)\n",
        "# H = model.fit(\n",
        "# \tx=aug.flow(trainX, trainY, batch_size=BS),\n",
        "# \tvalidation_data=(testX, testY),\n",
        "# \tsteps_per_epoch=len(trainX) // BS,\n",
        "# \tepochs=EPOCHS)\n",
        "\n"
      ],
      "metadata": {
        "id": "L6jhSDacXRqz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}