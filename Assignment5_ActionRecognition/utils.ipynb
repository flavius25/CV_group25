{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"utils.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["\"\"\" Mount Drive \"\"\"\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nEjcoY40txMe","outputId":"328e317b-b382-4564-af15-7cd8ecd9aa8e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import cv2\n","import numpy as np\n","import os\n","from tensorflow import keras\n","from keras.preprocessing.image import ImageDataGenerator\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from keras.models import Sequential\n","from keras import layers\n","import pandas as pd\n","from collections import deque\n","import copy\n","from sklearn.utils import shuffle\n","from keras.utils import np_utils\n","from google.colab.patches import cv2_imshow\n","from natsort import natsorted\n"],"metadata":{"id":"NUtFLmP_Eq64"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gV-2Ik_kfxmV"},"source":["### Stanford 40"]},{"cell_type":"code","metadata":{"id":"HhJtnW9eCTHd"},"source":["def onceSF():\n","  #Import dataset\n","  !wget http://vision.stanford.edu/Datasets/Stanford40_JPEGImages.zip\n","  !wget http://vision.stanford.edu/Datasets/Stanford40_ImageSplits.zip\n","\n","  #Unzip it\n","  !unzip Stanford40_JPEGImages.zip -d Stanford40/\n","  !unzip Stanford40_ImageSplits.zip -d Stanford40/\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XECyi77uk9Tf"},"source":["### TV Human Interaction (TV-HI)"]},{"cell_type":"code","source":["def onceTVHI():\n","\n","  #Download data\n","  !wget http://www.robots.ox.ac.uk/~alonso/data/tv_human_interactions_videos.tar.gz\n","  !wget http://www.robots.ox.ac.uk/~alonso/data/readme.txt\n","\n","  #Untar compressed files and move the readme.txt into TV-HI folder\n","  !mkdir TVHI_data\n","  !tar -xvf  'tv_human_interactions_videos.tar.gz' -C TVHI_data\n","  !mv readme.txt 'TV-HI/readme.txt'"],"metadata":{"id":"9L_i9Lgq_BRK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Main body utils###"],"metadata":{"id":"tyPoANBbBqNr"}},{"cell_type":"code","source":["\"\"\" Function for sorting the Stanford40 data in a way that can be accessed by Keras data loading function \"\"\"\n","\n","def dataExtractionSF(needDirectories):\n","    onceSF()\n","\n","    with open('Stanford40/ImageSplits/train.txt', 'r') as f:\n","        train_files = list(map(str.strip, f.readlines()))\n","        train_labels = ['_'.join(name.split('_')[:-1]) for name in train_files]\n","    \n","    \n","    with open('Stanford40/ImageSplits/test.txt', 'r') as f:\n","        test_files = list(map(str.strip, f.readlines()))\n","        test_labels = ['_'.join(name.split('_')[:-1]) for name in test_files]\n","\n","        \n","    action_categories = sorted(list(set(['_'.join(name.split('_')[:-1]) for name in train_files])))  \n","\n","    #Split training data here \n","    train_files, validation_files, train_labels, validation_labels = train_test_split(train_files, train_labels, test_size=0.1, random_state=0, stratify=train_labels)\n","\n","    if needDirectories:\n","\n","        print(\"Beginning sorting images...\")\n","            # Specify names of directories for train and test data\n","        dirs_needed = [\"SF_train\", \"SF_test\", \"SF_validation\"]\n","        files_n_labels = [[train_files, train_labels], [test_files, test_labels],[validation_files, validation_labels]]\n","\n","        for s in range(len(dirs_needed)):\n","\n","            os.mkdir(dirs_needed[s]) # make directory each for training and test set\n","\n","            for label in action_categories:\n","                os.mkdir(f\"{dirs_needed[s]}/{label}\") # in each directory make directories for all categories\n","\n","            counter = 0\n","            #Loop through all images and place them in the correct folder\n","            for file in range(len(files_n_labels[s][0])):\n","                label = files_n_labels[s][1][file]\n","                image = cv2.imread(f\"Stanford40/JPEGImages/{files_n_labels[s][0][file]}\")\n","                image_name = f\"{files_n_labels[s][1][file]}_{counter}.jpg\"\n","                print(image_name, label)\n","                path = f'./{dirs_needed[s]}/{label}'\n","                counter += 1\n","                cv2.imwrite(os.path.join(path,image_name), image) #Write image to directory \n","\n","        print(\"Done sorting images!\")\n","\n","    return train_labels, test_labels, validation_labels, action_categories\n","\n","\n","\"\"\" Load the Standford40 dataset \"\"\"\n","\n","def loadSF40(img_size=(224,224), needDirectories=False):\n","\n","    train_labels, test_labels, validation_labels, class_names = dataExtractionSF(needDirectories)\n"," \n","    train_ds = keras.utils.image_dataset_from_directory(\n","    directory='SF_train/',\n","    labels='inferred',\n","    label_mode='int',\n","    batch_size=32,\n","    image_size=img_size,\n","    shuffle=True\n","    )\n","\n","    val_ds = keras.utils.image_dataset_from_directory(\n","    directory='SF_validation/',\n","    labels='inferred',\n","    label_mode='int',\n","    batch_size=32,\n","    image_size=img_size,\n","    shuffle=True\n","    )\n","\n","    test_ds = keras.utils.image_dataset_from_directory(\n","    directory='SF_test/',\n","    labels='inferred',\n","    label_mode='int',\n","    batch_size=32,\n","    image_size=img_size\n","    )\n","    \n","    return train_ds, test_ds, val_ds, train_labels, test_labels, validation_labels, class_names\n","\n"],"metadata":{"id":"bPpK5_ygZ8Wy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\"\"\" Do dataExtraction on TVHI dataset, get the middle frame and sort into directories for easier data loading \"\"\"\n","\n","def dataExtractionTVHI(needDirectories):\n","    onceTVHI()\n","    \n","    set_1_indices = [[2,14,15,16,18,19,20,21,24,25,26,27,28,32,40,41,42,43,44,45,46,47,48,49,50],\n","                    [1,6,7,8,9,10,11,12,13,23,24,25,27,28,29,30,31,32,33,34,35,44,45,47,48],\n","                    [2,3,4,11,12,15,16,17,18,20,21,27,29,30,31,32,33,34,35,36,42,44,46,49,50],\n","                    [1,7,8,9,10,11,12,13,14,16,17,18,22,23,24,26,29,31,35,36,38,39,40,41,42]]\n","    set_2_indices = [[1,3,4,5,6,7,8,9,10,11,12,13,17,22,23,29,30,31,33,34,35,36,37,38,39],\n","                    [2,3,4,5,14,15,16,17,18,19,20,21,22,26,36,37,38,39,40,41,42,43,46,49,50],\n","                    [1,5,6,7,8,9,10,13,14,19,22,23,24,25,26,28,37,38,39,40,41,43,45,47,48],\n","                    [2,3,4,5,6,15,19,20,21,25,27,28,30,32,33,34,37,43,44,45,46,47,48,49,50]]\n","    classes = ['handShake', 'highFive', 'hug', 'kiss']  # we ignore the negative class\n","\n","    # test set\n","    test_files = [f'{classes[c]}_{i:04d}.avi' for c in range(len(classes)) for i in set_1_indices[c]]\n","    test_labels = [f'{classes[c]}' for c in range(len(classes)) for i in set_1_indices[c]]\n","   \n","    # training set\n","    train_files = [f'{classes[c]}_{i:04d}.avi' for c in range(len(classes)) for i in set_2_indices[c]]\n","    train_labels = [f'{classes[c]}' for c in range(len(classes)) for i in set_2_indices[c]]\n","     \n","    #Split training data here \n","    train_files, validation_files, train_labels, validation_labels = train_test_split(train_files, train_labels, test_size=0.15, random_state=0, stratify=train_labels)\n","\n","\n","    if needDirectories:\n","        \n","        print(\"Beginning sorting images...\")\n","            # Specify names of directories for train, validation and test data\n","        dirs_needed = [\"TVHI_train\", \"TVHI_test\", \"TVHI_validation\"]\n","        files_n_labels = [[train_files, train_labels], [test_files, test_labels],[validation_files, validation_labels]]\n","\n","        for s in range(len(dirs_needed)):\n","\n","            os.mkdir(dirs_needed[s]) # make directory each for training, validation and test sets\n","\n","            for label in classes:\n","                os.mkdir(f\"{dirs_needed[s]}/{label}\") # in each directory make directories for all categories\n","\n","            counter = 0\n","            #Loop through all videos, take middle frame and place them in the correct folder\n","            for video in range(len(files_n_labels[s][0])):\n","                label = files_n_labels[s][1][video]\n","                vidcap = cv2.VideoCapture(f'TVHI_data/tv_human_interactions_videos/{files_n_labels[s][0][video]}')\n","                middle_frame = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT)/2)\n","                vidcap.set(cv2.CAP_PROP_POS_FRAMES, middle_frame) #Get the middle frame of the video\n","                success, frame = vidcap.read()\n","                image_name = f\"{files_n_labels[s][1][video]}_{counter}.jpg\"\n","                print(image_name, label)\n","                path = f'./{dirs_needed[s]}/{label}'\n","                counter += 1\n","                cv2.imwrite(os.path.join(path,image_name), frame) #Write image to directory \n","\n","        print(\"Done sorting images!\")\n","    \n","    \n","    return train_labels, test_labels, validation_labels, classes\n","\n","def loadTVHI(img_size=(224,224), needDirectories=False):\n","\n","    train_labels, test_labels, validation_labels, class_names = dataExtractionTVHI(needDirectories)\n"," \n","    train_ds = keras.utils.image_dataset_from_directory(\n","    directory='TVHI_train/',\n","    labels='inferred',\n","    label_mode='int',\n","    batch_size=17,\n","    image_size=img_size,\n","    shuffle=True\n","    )\n","\n","    val_ds = keras.utils.image_dataset_from_directory(\n","    directory='TVHI_validation/',\n","    labels='inferred',\n","    label_mode='int',\n","    batch_size=15,\n","    image_size=img_size,\n","    shuffle=True\n","    )\n","\n","    test_ds = keras.utils.image_dataset_from_directory(\n","    directory='TVHI_test/',\n","    labels='inferred',\n","    label_mode='int',\n","    batch_size=14,\n","    image_size=img_size\n","    )\n","    \n","    return train_ds, test_ds, val_ds, train_labels, test_labels, validation_labels, class_names\n"],"metadata":{"id":"cumo8sDxtLV5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\" the Optical Flow input to the CNN \"\"\"\n","\n","def opticalFlowDataExtraction(IMG_SIZE=(224,224)):\n","    onceTVHI()\n","\n","    #Take relevant data and create test and training set (set 1 = test set,set 2 = training set)\n","    set_1_indices = [[2,14,15,16,18,19,20,21,24,25,26,27,28,32,40,41,42,43,44,45,46,47,48,49,50],\n","                 [1,6,7,8,9,10,11,12,13,23,24,25,27,28,29,30,31,32,33,34,35,44,45,47,48],\n","                 [2,3,4,11,12,15,16,17,18,20,21,27,29,30,31,32,33,34,35,36,42,44,46,49,50],\n","                 [1,7,8,9,10,11,12,13,14,16,17,18,22,23,24,26,29,31,35,36,38,39,40,41,42]]\n","    set_2_indices = [[1,3,4,5,6,7,8,9,10,11,12,13,17,22,23,29,30,31,33,34,35,36,37,38,39],\n","                    [2,3,4,5,14,15,16,17,18,19,20,21,22,26,36,37,38,39,40,41,42,43,46,49,50],\n","                    [1,5,6,7,8,9,10,13,14,19,22,23,24,25,26,28,37,38,39,40,41,43,45,47,48],\n","                    [2,3,4,5,6,15,19,20,21,25,27,28,30,32,33,34,37,43,44,45,46,47,48,49,50]]\n","    classes = ['handShake', 'highFive', 'hug', 'kiss']  # we ignore the negative class\n"," \n","    # test set\n","    test_files = [f'{classes[c]}_{i:04d}.avi' for c in range(len(classes)) for i in set_1_indices[c]]\n","    test_labels = [f'{classes[c]}' for c in range(len(classes)) for i in set_1_indices[c]]\n","   \n","    # training set\n","    train_files = [f'{classes[c]}_{i:04d}.avi' for c in range(len(classes)) for i in set_2_indices[c]]\n","    train_labels = [f'{classes[c]}' for c in range(len(classes)) for i in set_2_indices[c]]\n","     \n","    #Split training data here \n","    train_files, validation_files, train_labels, validation_labels = train_test_split(train_files, train_labels, test_size=0.15, random_state=0, stratify=train_labels)\n","\n","    \n","    print(\"Beginning sorting images...\")\n","        # Specify names of directories for train, validation and test data\n","    dirs_needed = [\"OF_train\", \"OF_test\", \"OF_validation\"]\n","    files_n_labels = [[train_files, train_labels], [test_files, test_labels],[validation_files, validation_labels]]\n","\n","    for s in range(len(dirs_needed)):\n","\n","        os.mkdir(dirs_needed[s]) # make directory each for training, validation and test sets\n","\n","        for c_lab in classes:\n","            os.mkdir(f\"{dirs_needed[s]}/{c_lab}\") # in each directory make directories for all categories\n","\n","        counter1 = 0\n","        #Loop through all videos, take middle frame and place them in the correct folder\n","        for video in range(len(files_n_labels[s][0])):\n","            video_name = f\"{files_n_labels[s][0][video][:-4]}\"\n","            label = files_n_labels[s][1][video]\n","            print(\"video_name:  \", video_name)\n","            os.mkdir(f\"{dirs_needed[s]}/{label}/{video_name}\")\n","            vidcap = cv2.VideoCapture(f'TVHI_data/tv_human_interactions_videos/{files_n_labels[s][0][video]}')\n","            starting_frame = int((vidcap.get(cv2.CAP_PROP_FRAME_COUNT)/2)-8)\n","            vidcap.set(cv2.CAP_PROP_POS_FRAMES, starting_frame) #Get the middle frame of the video\n","            success, old_frame = vidcap.read()\n","            #preprocess image\n","            hsv = np.zeros_like(old_frame) \n","            hsv[...,1] = 255                                                # Set HSV's Value-channel to constant\n","            old_frame = cv2.cvtColor(old_frame, cv2.COLOR_BGR2GRAY)         # Convert to grayscale to fit algorithm (Farneback)\n","\n","            counter = 0\n","            for i in range(16): #Loop over 16 frames, middle frame will be middle of stack\n","\n","                success, new_frame = vidcap.read()\n","                if not success:\n","                    break\n","                \n","                #Do preprocessing of new frame \n","                new_frame  = cv2.cvtColor(new_frame,cv2.COLOR_BGR2GRAY)\n","\n","                flow = cv2.calcOpticalFlowFarneback(old_frame,new_frame, None, 0.5, 3, 15, 3, 5, 1.2, 0)   # calculate the optical flow for each pixel in the frame with Farneback\n","                \n","                mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])       # find magnitude and direction and encode it in an image\n","                hsv[..., 0] = ang*180/np.pi/2\n","                hsv[..., 2] = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX)\n","                bgr = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n","                of_frame  = cv2.resize(bgr, IMG_SIZE)                   # Resize image to fit the other data\n","                frame_name = f\"{video_name}_{counter}.jpg\"\n","                path = f'./{dirs_needed[s]}/{label}/{video_name}'\n","                #print(\"path: \", path)\n","                cv2.imwrite(os.path.join(path,frame_name), of_frame) #Write image to directory\n","                counter += 1 \n","            \n","            print(f\"{video_name} , {label}\")\n","            counter1 += 1 \n","\n","    print(\"Done sorting images!\")\n","\n","def toCSVconverter():\n","    num_classes = 4\n","    labels_name = {'handShake' : 0, 'highFive': 1, 'hug' : 2, 'kiss' : 3}\n","\n","    dirs = [\"OF_train\", \"OF_test\", \"OF_validation\"]\n","\n","    os.mkdir(\"data_files\")\n","    for i in dirs:\n","        os.mkdir(f\"data_files/{i}\")\n","        train_data_path = f\"{i}\"\n","    \n","        data_dir_list = os.listdir(train_data_path)\n","        print(data_dir_list)\n","        for data_dir in data_dir_list:\n","            label = labels_name[str(data_dir)]\n","            video_list = os.listdir(os.path.join(train_data_path, data_dir))\n","            print(video_list)\n","            for vid in video_list: # Loop over each video\n","                train_df = pd.DataFrame(columns=[\"FileName\", \"Label\", \"ClassName\"], dtype=object) #Create dataframe for each video\n","                img_list = os.listdir(os.path.join(train_data_path, data_dir, vid))\n","                for img in img_list:\n","                    img_path = os.path.join(train_data_path, data_dir, vid, img) #get the image path for each frame and append it to the created dataframe\n","                    train_df = train_df.append({\"FileName\": img_path, \"Label\" : label, \"ClassName\" : data_dir}, ignore_index=True)\n","                file_name = f\"{data_dir}_{vid}.csv\"\n","                sorted_df = train_df.sort_values(by=['FileName'], ascending=True)\n","                train_df.to_csv(f\"data_files/{i}/{file_name}\")\n","\n","\"\"\" Load Optical Flow data as data generator which inputs 16 images as 1 input \"\"\"\n","\n","def loadOFdatagens():\n","\n","    train_data = seq_of_frames(\"OF_train\", 16)\n","    test_data = seq_of_frames(\"OF_test\", 16)\n","    val_data = seq_of_frames(\"OF_validation\", 16)\n","\n","    train_gen = data_generator(train_data,batch_size=17, shuffle=False)\n","    val_gen = data_generator(test_data,batch_size=15, shuffle=False)\n","    test_gen = data_generator(val_data,batch_size=14, shuffle=False)\n","\n","    print(\"Success!\")\n","\n","    return train_gen, val_gen, test_gen\n","\n","def filegenerator(data_path, data_files,temporal_length):\n","    ## Creates a python generator that 'yields' a sequence of frames every time based on the temporal length and stride.\n","    for file in data_files:\n","        data = pd.read_csv(os.path.join(data_path, file))\n","        labels = list(data.Label)\n","        total_images = len(labels)\n","        if total_images == temporal_length:\n","            img_list = list(data.FileName)\n","            natsorted(img_list)\n","        else:\n","            continue\n","        \n","        samples = deque()\n","        samp_count = 0\n","        \n","        for img in img_list:\n","            samples.append(img)\n","\n","            if len(samples)== temporal_length: \n","                samples_c = copy.deepcopy(samples)\n","                samp_count += 1\n","                yield samples_c,labels[0]\n","\n","##Function to create the files structured based on the temporal requirements.:\n","def seq_of_frames(folder, length):\n","    \n","    data_path = os.path.join(\"/content/drive/MyDrive/Colab Notebooks/OF_data/data_files\", folder)\n","    data_files = os.listdir(data_path)\n","    \n","    file_gen = filegenerator(data_path, data_files , length)\n","    iterator = True\n","    data_list = []\n","    while iterator:\n","        try:\n","            X,y = next(file_gen)\n","            X = list(X) \n","            data_list.append([X,y])\n","        except Exception as e:\n","            print(\"StopIteration exception:\",e)\n","            iterator = False \n","    \n","    print(\"length data_files: \", len(data_files))       \n","    print(\"len data_list : \", len(data_list))\n","\n","    return data_list\n","\n","def shuffle_data(samples):\n","    data = shuffle(samples,random_state=2)\n","    return data\n","\n","def preprocess_image(img):\n","    img = img/255\n","    return img\n","\n","def data_generator(data,batch_size=17,shuffle=True):   \n","          \n","    \"\"\"\n","    Yields the next training batch.\n","    data is an array [[img1_filename,img2_filename...,img16_filename],label1], [image2_filename,label2],...].\n","    \"\"\"\n","    link_to_data = \"/content/drive/MyDrive/Colab Notebooks/OF_data/\"\n","    num_samples = len(data)\n","    if shuffle:\n","        data = shuffle_data(data)\n","    while True:   \n","        for offset in range(0, num_samples, batch_size):\n","            #print ('startring index: ', offset) \n","            # Get the samples you'll use in this batch\n","            batch_samples = data[offset:offset+batch_size]\n","            # Initialise X_train and y_train arrays for this batch\n","            X_train = []\n","            y_train = []\n","            # For each example\n","            for batch_sample in batch_samples:\n","                # Load image (X)\n","                x = batch_sample[0]\n","                y = batch_sample[1]\n","                temp_data_list = []\n","                for img in x:\n","                    try:\n","                        img = str(img)\n","                        img = cv2.imread(link_to_data+img)\n","                        #apply any kind of preprocessing here\n","                        img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n","                        img = preprocess_image(img)\n","                        temp_data_list.append(img)\n","\n","                    except Exception as e:\n","                        print (e)\n","                        print ('error reading file: ',img)  \n","\n","                # Read label (y)\n","                #label = label_names[y]\n","                # Add example to arrays\n","                X_train.append(temp_data_list)\n","                y_train.append(y)\n","    \n","            # Make sure they're numpy arrays (as opposed to lists)\n","            X_train = np.array(X_train).astype('float32')\n","            #X_train = np.rollaxis(X_train,1,4)\n","            y_train = np.array(y_train).astype('float32')\n","            y_train = np_utils.to_categorical(y_train, num_classes=4).astype('float32')\n","\n","            # The generator-y part: yield the next training batch            \n","            yield X_train, y_train"],"metadata":{"id":"nNhSdAnr3L67"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\"\"\"   Data augmentation and Normalisation \"\"\"\n","def dataAugmentation():\n","\n","    img_augmentation = Sequential(\n","    [\n","        #layers.Rescaling(scale=1./255),\n","        layers.RandomRotation(factor=0.15),\n","        layers.RandomTranslation(height_factor=0.1, width_factor=0.1),\n","        layers.RandomFlip(mode=\"horizontal\"),\n","        layers.RandomContrast(factor=0.1),\n","        layers.RandomZoom(0.1)\n","    ],\n","    name=\"img_augmentation\",\n","    )\n","\n","    return img_augmentation\n","\n","\n","\"\"\" Function for plotting accuracy\"\"\"\n","\n","def plotAccuracy(title, train_acc, val_acc):\n","    plt.title(title)\n","    plt.plot(train_acc)\n","    plt.plot(val_acc)\n","    plt.ylabel(\"Accuracy\")\n","    plt.xlabel(\"Epochs\")\n","    plt.legend(['train', 'val'], loc = 'upper left')\n","    plt.ylim([0, 1])\n","    plt.show()\n","\n","\"\"\" Function for plotting loss\"\"\"\n","\n","def plotLoss(title, train_loss, val_loss):\n","    plt.title(title)\n","    plt.plot(train_loss)\n","    plt.plot(val_loss)\n","    plt.ylabel(\"Loss\")\n","    plt.xlabel(\"Epochs\")\n","    plt.legend(['train', 'val'], loc = 'lower left')\n","    plt.ylim([0, 5])\n","    plt.show()\n"],"metadata":{"id":"J8KpxkDQtEiO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#https://stackoverflow.com/questions/59492866/keras-imagedatagenerator-for-multiple-inputs-and-image-based-target-output"],"metadata":{"id":"Azbtl2gozBxr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\" Generator for the multiple inputs \"\"\"\n","\n","def joinedGens(Train=False, Val=False, Test=False, needDirectories=False):\n","\n","  #Get the different generators\n","  train_ds, test_ds, val_ds, train_labels, test_labels, validation_labels, class_names = loadTVHI(img_size=(224,224), needDirectories=needDirectories)\n","  train_gen, val_gen, test_gen = loadOFdatagens()\n","\n","  if Train:\n","    gen1 = train_ds\n","    gen2 = train_gen\n","  \n","  elif Val:\n","    gen1 = val_ds\n","    gen2 = val_gen\n","  \n","  elif Test:\n","    gen1 = test_ds\n","    gen2 = test_gen\n","  \n","  else:\n","    print(\"Please specify which part of the dataset (training | validation | test) the data belongs to\")\n","      \n","  \n","  #Yield data generators depending on the which dataset\n","  combo_gen = map(format_gen_outputs, gen1, gen2)\n","  return combo_gen\n","\n"],"metadata":{"id":"af_S6-YgxNzJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\" Function to format the output of two generators\"\"\"\n","def format_gen_outputs(gen1,gen2):\n","  x1 = gen1[0]\n","  x2 = gen2[0]\n","  y1 = gen1[1]\n","  return [x1, x2], y1\n","\n"],"metadata":{"id":"eyMLEj3H5Z8G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# my_gen = generate_generator_multiple(generator_imgs)\n","# model.fit_generator(my_gen, ...)"],"metadata":{"id":"NK3eppE3zW93"},"execution_count":null,"outputs":[]}]}